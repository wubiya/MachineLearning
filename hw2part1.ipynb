{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Problem 0(Not graded): install keras, TensorFlow\n",
    "# Instruction can be found at:\n",
    "# http://keras.io/#installation\n",
    "# https://www.tensorflow.org/versions/master/get_started/os_setup.html#pip-installation\n",
    "# then add to your .bashrc\n",
    "# export KERAS_BACKEND=tensorflow\n",
    "# (Hint: this is all linux, if you have windows, then linux vm or see docker installation but it would be easier to run on linux.)\n",
    "\n",
    "# import keras\n",
    "from keras.models import Sequential\n",
    "from keras.objectives import mae\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l1,l2,l1l2\n",
    "from time import time\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from numpy import arange\n",
    "\n",
    "\n",
    "N,K = 1000,10\n",
    "np.random.seed(78) #Make sure to use seed 78 for grading\n",
    "def generate(N,K): #lazy generator\n",
    "    res = randn(N,K); #make N \n",
    "    res[:,0]=1; # replace first variable with constant\n",
    "    return res\n",
    "f = (lambda x: x.dot(arange(K)-1))\n",
    "noise_multiplier = 1\n",
    "x1 = generate(4*N,K)\n",
    "y1 = f(x1) + noise_multiplier * K * randn(4*N)\n",
    "x2 = generate(N,K)\n",
    "y2 = f(x2) + noise_multiplier * K * randn(N)\n",
    "x3 = generate(N,K)\n",
    "y3 = f(x3) + noise_multiplier * K * randn(N)\n",
    "\n",
    "def timed_sgd_OLS(x1,y1,x2=None,y2=None,\n",
    "                  lr=0.1, decay=1e-2, nesterov=True, momentum=0.8, batch_size=100,nb_epoch=10):\n",
    "    if (x2 is None)^(x2 is None): raise ValueError(\"if you specify x2 or y2 you need to specify the other as well\")\n",
    "    if x2 is None and y2 is None: x2=x1; y2=y1 #if no Cross-validation set, use original\n",
    "    time0 = time() # Start timer\n",
    "    sgd=SGD(lr=lr, decay=decay, nesterov=nesterov, momentum=momentum)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=K, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=sgd)\n",
    "    model.fit(x1, y1,nb_epoch=nb_epoch,batch_size=batch_size, show_accuracy=True)\n",
    "    score = model.evaluate(x2, y2, batch_size=20)\n",
    "    print(\"Score:\",score,'\\n',np.around(model.layers[0].get_weights()[0][:,0],2))\n",
    "    return time()-time0, score\n",
    "\n",
    "#Problem 1: Write analogous functions\n",
    "def timed_sgd_Ridge(x1,y1,x2=None,y2=None,\n",
    "                    lambda2=0.01, lr=0.1, decay=1e-2, nesterov=True, momentum=0.8, batch_size=100,nb_epoch=50):\n",
    "    pass\n",
    "\n",
    "def timed_sgd_LASSO(x1,y1,x2=None,y2=None,\n",
    "            lambda1=0.01, lr=0.1, decay=1e-2, nesterov=True, momentum=0.8, batch_size=100,nb_epoch=50):\n",
    "    pass\n",
    "\n",
    "def timed_sgd_Elastic(x1,y1,x2=None,y2=None,\n",
    "    lambda1=0.01, lambda2=0.01, lr=0.1, decay=1e-2, nesterov=True, momentum=0.8, batch_size=100,nb_epoch=50):\n",
    "    pass\n",
    "\n",
    "#Problem 2: \n",
    "#Part A: Your goal here is to obtain better time and score than default parameters for each of the four above functions:\n",
    "#timed_sgd_OLS, timed_sgd_Ridge, timed_sgd_LASSO, timed_sgd_Elastic when run with seed(78) and x1,y1,x2,y2 as defined above.\n",
    "#Part B: Same but with random seed instead of seed(78); if you did Part A right, this part\n",
    "#is trivial but you should be able to obtain better performance in almost every case.\n",
    "#You answer should show code by which you found these parameters with some brief explanation.\n",
    "\n",
    "#Suggested path: for each function using this seed(78) while keeping other parameters the same, find better lambda2,\n",
    "#then repeat same for lr, decay, nesterov, momentum, batch_size while each time updating previous parameters to newly\n",
    "#found better parameters.\n",
    "#For example you found that lambda2 is better to be 0.1,\n",
    "#then you optimize next lr while setting lambda to 0.1 and rest of parameters the same and if lr=0.01\n",
    "#is better, etc.  You can choose any other path \n",
    "#(EXTRA CREDIT WILL BE GIVEN AT DISCRETION FOR: showing application of any techniques from class to this problem; \n",
    "# obtaining results significantly better than median of the class; the goal is to be student with best accuracy \n",
    "# and reasonable time)\n",
    "\n",
    "#Problem 3: From running problem 2, make observation of what happens when you change each of the parameters:\n",
    "#a) Learning_rate (i.e. lr)\n",
    "#b) decay\n",
    "#c) momentum\n",
    "#d) Nesterov\n",
    "#e) batch_size\n",
    "#f) nb_epoch\n",
    "#one line explanation for each case suffices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 0s - loss: 114.8977 - acc: 0.0000e+00     \n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 0s - loss: 106.0919 - acc: 0.0000e+00     \n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 0s - loss: 105.3614 - acc: 0.0000e+00     \n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 0s - loss: 105.3231 - acc: 0.0000e+00     \n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 0s - loss: 104.5676 - acc: 0.0000e+00     \n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 0s - loss: 104.5838 - acc: 0.0000e+00     \n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 0s - loss: 104.7212 - acc: 0.0000e+00     \n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 0s - loss: 104.0495 - acc: 0.0000e+00     \n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 0s - loss: 104.4916 - acc: 0.0000e+00     \n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 0s - loss: 104.0366 - acc: 0.0000e+00     \n",
      "1000/1000 [==============================] - 0s     \n",
      "Score: 100.497054291 \n",
      " [-0.34        0.61000001  1.28999996  1.75        2.50999999  3.86999989\n",
      "  5.28999996  5.9000001   6.55000019  8.32999992]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23.819067001342773, 100.49705429077149)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timed_sgd_OLS(x1,y1,x2,y2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
